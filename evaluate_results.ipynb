{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fdcf23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7c79f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grpo_rag.utils.memory_buffer_utils import read_jsonl\n",
    "from grpo_rag.utils.constants import RESULTS_BASELINE_FILE, RESULTS_PATH, RESULTS_POLICY_FILE, MEMORY_BUFFER_PATH\n",
    "from grpo_rag.utils.grpo_utils import get_answer_length, get_grpo_advantage_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c71aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = read_jsonl(RESULTS_PATH + RESULTS_BASELINE_FILE)\n",
    "policy_results= read_jsonl(RESULTS_PATH + RESULTS_POLICY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64d36b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_answer_length = [len(result['answer'].split()) for result in baseline_results][-10:]\n",
    "policy_answers_cleaned = [result['answer'].split('<answer>')[1] for result in policy_results[1:]][-10:]\n",
    "policy_reasons_cleaned = [result['answer'].split('<answer>')[0] for result in policy_results[1:]][-10:]\n",
    "\n",
    "policy_answer_length = [len(result.split()) for result in policy_answers_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53a40139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policy_reasons_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c27a7a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=np.float64(0.3377576027979062), pvalue=np.float64(0.7394529715158317), df=np.float64(18.0))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(baseline_answer_length, policy_answer_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4665b91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(190.8)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline_answer_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b993fb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(184.0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(policy_answer_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f9bba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reason>  \n",
      "To derive the answering policy, let’s analyze the example groups:\n",
      "\n",
      "1. High-scoring answers often:  \n",
      "   - Begin with a direct, concise definition that situates the concept within its field.\n",
      "   - Use simple, accessible language.\n",
      "   - List key characteristics or components in a clear, bulleted or separated format.\n",
      "   - Give 1-2 practical examples or common use cases (if relevant) to ground understanding.\n",
      "   - Summarize or reinforce the main point at the end in plain terms.\n",
      "   - Avoid excessive repetition or verbosity.\n",
      "\n",
      "2. Lower scoring answers tend to:  \n",
      "   - Repeat phrases or information.\n",
      "   - Provide overly detailed breakdowns without adding clarity.\n",
      "   - Skip contextual applications or real-world analogies.\n",
      "   - Lack an explicit summary or closing sentence.\n",
      "\n",
      "**Policy (inferred):**\n",
      "- Start with a direct, plain-language definition.\n",
      "- Explain clearly, using bullet points for features/implications when helpful.\n",
      "- Provide a simple example for real-world context if applicable.\n",
      "- End with a summarizing statement.\n",
      "- Keep it focused, concise, and avoid unnecessary detail or repetition.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(policy_reasons_cleaned[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4cefd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_entries = read_jsonl(MEMORY_BUFFER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b355b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are some common techniques for feature engineering?',\n",
       " 'answers': ['Here are common feature-engineering techniques, grouped by purpose, with short notes on when and why to use them.\\n\\nBasic cleaning and preprocessing\\n- Missing-value handling: impute (mean/median/mode), model-based imputation, or add “missing” indicator features. Missingness itself can be predictive.\\n- Outlier treatment: clip, winsorize, or transform extreme values; or create an “is_outlier” flag.\\n- Type conversion: ensure numeric vs categorical vs datetime are correct.\\n\\nEncoding categorical variables\\n- One-hot / dummy encoding: for low-cardinality categories and linear models.\\n- Ordinal encoding: for inherently ordered categories.\\n- Frequency / count encoding: replace category by its frequency; simple and robust.\\n- Target / mean encoding (or smoothing): replace by conditional target mean; powerful for high-cardinality features but risk of leakage—use proper cross-validation or regularization.\\n- Feature hashing: hash categories to a fixed-size vector when cardinality is huge.\\n\\nScaling and normalization\\n- Standardization (z-score): useful for linear models, k-NN, SVM.\\n- Min–max scaling: keeps values in [0,1], helpful for neural nets.\\n- Robust scaling (median and IQR): reduces influence of outliers.\\n- Power transforms (log, Box–Cox, Yeo–Johnson): make skewed distributions more Gaussian-like.\\n\\nTransformations and nonlinear features\\n- Polynomial features and interactions: squares, products, cross-terms to capture nonlinear relationships.\\n- Log, reciprocal, sqrt: stabilize variance or linearize relationships.\\n- Binning / discretization: convert numeric to categories (equal-width, equal-frequency, or custom); can help with nonlinearity and robustness.\\n\\nTime-series and temporal features\\n- Date/time decomposition: hour, day, month, weekday, is_weekend, quarter, etc.\\n- Lag features and rolling statistics: previous values, rolling mean/std, exponentially weighted, useful for forecasting.\\n- Time since event / time to next event.\\n\\nAggregation and grouping\\n- Group-by aggregations: mean, sum, count, std per group (e.g., user-level or product-level features).\\n- Cross-entity features: ratios between aggregates (e.g., user_purchase_amount / user_avg_purchase).\\n\\nText feature engineering\\n- Bag-of-words, TF-IDF, n-grams: classic sparse representations.\\n- Embeddings: word/sentence embeddings (Word2Vec, BERT) or pretrained models for dense features.\\n- Simple text features: length, number of stopwords, punctuation counts, sentiment scores.\\n\\nImage and multimedia\\n- Pretrained CNN embeddings: use a network’s penultimate layer as features.\\n- Basic image stats: color histograms, texture measures, aspect ratio.\\n\\nDimensionality reduction and selection\\n- PCA, ICA: reduce dimensionality while preserving variance.\\n- LDA (for supervised projection), UMAP/t-SNE (visualization).\\n- Feature selection: filter methods (correlation, chi-square), wrapper methods (RFE), embedded methods (L1 regularization, tree feature importances).\\n\\nSpecial encodings and patterns\\n- Cyclical features as sin/cos transforms: for hours, months, angles to preserve continuity.\\n- Feature crosses: combine categorical features into new categories (useful in recommender systems).\\n- Ratios and differences: often more informative than raw counts.\\n\\nModel-specific notes\\n- Tree-based models: handle unscaled data and some missing values; focus on categorical encodings and interaction features.\\n- Linear/Distance-based models: need careful scaling and explicit nonlinear features or interactions.\\n\\nPractical workflow and cautions\\n- Avoid data leakage: compute encodings/aggregations only on training folds when using target-based methods.\\n- Use pipelines: keep preprocessing consistent across training and deployment.\\n- Validate features: measure impact via cross-validation, feature importance, or ablation studies.\\n- Keep features interpretable when possible; simpler features often generalize better.\\n\\nIf you want, tell me your data type (tabular, text, time series, images) and model class, and I’ll suggest specific feature ideas and priorities.',\n",
       "  'Here are common feature-engineering techniques, grouped by type, with short explanations and when to use them:\\n\\nBasic preprocessing\\n- Imputation: fill missing values (mean/median/mode, KNN, model-based). Use to avoid dropping rows/columns.\\n- Scaling/normalization: standardize (z-score), min–max, robust scaling. Important for distance-based models and ones sensitive to feature scale (SVM, KNN, neural nets).\\n- Outlier handling: remove, cap (winsorize), or transform outliers. Helps robustify models.\\n\\nEncoding categorical variables\\n- One-hot encoding: for low-cardinality nominal categories.\\n- Ordinal encoding: when categories have a natural order.\\n- Frequency/count encoding: replace category with its frequency or count; helps high-cardinality cases.\\n- Target/mean encoding: replace category with target mean (use careful regularization and CV to avoid leakage).\\n- Feature hashing: memory-efficient for very high-cardinality text-like categories.\\n\\nCreation and transformation\\n- Interaction features: multiply or combine features (x1*x2, x1/x2). Useful when effects are non-additive.\\n- Polynomial features: include squared, cubic, etc., to capture nonlinearities for linear models.\\n- Log/Box-Cox/Yeo-Johnson transforms: stabilize variance and reduce skew.\\n- Binning/discretization: convert continuous to categorical (equal-width, equal-frequency, domain-driven). Can improve robustness or capture thresholds.\\n\\nTime-series and lagged features\\n- Lag features: previous values (t-1, t-24) for forecasting.\\n- Rolling/window statistics: rolling mean, std, min/max, counts.\\n- Time components: hour, day, weekday, season, elapsed time.\\n- Fourier/spectral features: capture periodicity.\\n\\nAggregation and entity features (for relational or grouped data)\\n- Group-by aggregations: mean, sum, count, std, min, max per group (user, product).\\n- Ratios/derived metrics: e.g., clicks/visits, revenue/transactions.\\n- Cross-entity features: merge statistics from related tables.\\n\\nText feature extraction\\n- Bag-of-words / TF-IDF / n-grams: common baseline for NLP.\\n- Embeddings: pretrained word/sentence embeddings (Word2Vec, BERT) for semantic features.\\n- Domain-specific features: length, punctuation counts, sentiment, POS tags, named-entity counts.\\n\\nImage feature extraction\\n- Basic: color histograms, texture descriptors (LBP), shape features.\\n- Deep features: activations from pretrained CNNs (transfer learning).\\n\\nDimensionality reduction and feature extraction\\n- PCA, SVD: reduce dimensionality while preserving variance (good for noise reduction).\\n- LDA (for text), t-SNE/UMAP (visualization), autoencoders (nonlinear reduction).\\n\\nFeature selection and regularization\\n- Filter methods: correlation, mutual information, chi-square to prefilter features.\\n- Wrapper methods: recursive feature elimination, forward/backward selection (model-based).\\n- Embedded methods: feature importance from tree models, L1 (Lasso) regularization.\\n- Use cross-validation to avoid selection bias.\\n\\nAutomated and engineered feature tools\\n- Automated feature generation: Featuretools (deep feature synthesis), tsfresh for time-series.\\n- Feature hashing, pipelines, and transformation chaining for reproducibility.\\n\\nPractical tips and best practices\\n- Prevent leakage: ensure features are computed using only training data (use proper CV/time-based splits).\\n- Use pipelines: apply transformations consistently and avoid data leakage.\\n- Start simple: baseline with basic transforms, then add complexity iteratively and validate with CV.\\n- Evaluate incremental value: measure how much each engineered feature improves validation metrics.\\n- Watch sparsity and cardinality: one-hot a huge cardinality can blow up dimensions—consider hashing, embeddings, or target encoding.\\n\\nCommon libraries\\n- pandas, scikit-learn (preprocessing, pipelines, feature selection), category_encoders, Featuretools, tsfresh, nltk/spacy, gensim, Hugging Face transformers for embeddings.\\n\\nIf you want, tell me your dataset type (tabular, time-series, text, images) and task (classification, regression, forecasting), and I can suggest a concrete feature-engineering plan.'],\n",
       " 'advantage_scores': [1.0, -1.0],\n",
       " 'gold_answer': 'Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22b4128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short definition\n",
      "- Regularization adds constraints or penalties during training so the network learns simpler, more robust functions that generalize better, reducing overfitting (high variance).\n",
      "\n",
      "How regularization prevents overfitting — mechanisms\n",
      "- Weight decay / L2 regularization\n",
      "  - Adds λ||w||^2 to the training loss. This penalizes large weights and biases the model toward smaller weights.\n",
      "  - Effect: smoother functions, less sensitivity to noise, reduced effective capacity → lower variance.\n",
      "- L1 regularization\n",
      "  - Adds λ||w||1 to the loss; encourages sparse weights (many near-zero), which can effectively remove irrelevant features and simplify the model.\n",
      "- Dropout\n",
      "  - Randomly zeroes activations during training; each update trains a different sub-network.\n",
      "  - Effect: prevents co-adaptation of units, acts like training an ensemble of thinned networks, reduces reliance on any single neuron and improves robustness.\n",
      "- Early stopping\n",
      "  - Stop training when validation loss stops improving. Prevents the model from continuing to fit noise in the training set.\n",
      "- Data augmentation (often used as regularization)\n",
      "  - Increasing input variety forces the model to learn invariances instead of memorizing examples.\n",
      "- Output/label regularization\n",
      "  - Label smoothing discourages overconfident predictions and yields softer, better-calibrated outputs that generalize better.\n",
      "- Architectural and norm-based constraints\n",
      "  - Max-norm or clipping, spectral norm constraints, and limiting network width/depth reduce capacity and stabilize training.\n",
      "- Implicit regularization (optimization and normalization)\n",
      "  - Optimizers, batch normalization, and training procedures can implicitly regularize (e.g., SGD noise tends to prefer flatter minima).\n",
      "\n",
      "Intuition in one line\n",
      "- Regularization reduces the model’s ability to fit spurious patterns in the training data (reduces variance) while preserving its ability to capture the true signal.\n",
      "\n",
      "Practical guidance\n",
      "- Start with weight decay (e.g., 1e-4 to 1e-3 for many CNNs) and data augmentation for vision tasks.\n",
      "- Use dropout (0.1–0.5) in fully connected layers or where overfitting is observed; use cautiously with batch norm (they interact).\n",
      "- Monitor validation metrics and use early stopping based on the validation set.\n",
      "- Combine methods: e.g., weight decay + augmentation + early stopping (and dropout only where appropriate).\n",
      "- Tune regularization strength via validation or cross-validation; too much regularization causes underfitting (high bias).\n",
      "\n",
      "Summary\n",
      "- Regularization constrains the hypothesis space or training dynamics so the network learns simpler, smoother, and more robust functions, which reduces variance and helps the model generalize to unseen data.\n"
     ]
    }
   ],
   "source": [
    "print(policy_results[5]['answer'].split('<answer>')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66d14f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reason>\n",
      "Policy derived from examples:\n",
      "- Be concise and clear; start with a short definition, then explain mechanisms and practical implications.\n",
      "- Use bullet points for key mechanisms and trade-offs.\n",
      "- Give brief formulas or intuitive math where helpful (but keep it short).\n",
      "- End with practical recommendations (typical choices, how to combine, what to monitor).\n",
      "- Avoid heavy formatting; use plain text and short paragraphs.\n",
      "</reason>\n",
      "\n",
      "<answer>\n",
      "Short definition\n",
      "- Regularization adds constraints or penalties during training so the network learns simpler, more robust functions that generalize better, reducing overfitting (high variance).\n",
      "\n",
      "How regularization prevents overfitting — mechanisms\n",
      "- Weight decay / L2 regularization\n",
      "  - Adds λ||w||^2 to the training loss. This penalizes large weights and biases the model toward smaller weights.\n",
      "  - Effect: smoother functions, less sensitivity to noise, reduced effective capacity → lower variance.\n",
      "- L1 regularization\n",
      "  - Adds λ||w||1 to the loss; encourages sparse weights (many near-zero), which can effectively remove irrelevant features and simplify the model.\n",
      "- Dropout\n",
      "  - Randomly zeroes activations during training; each update trains a different sub-network.\n",
      "  - Effect: prevents co-adaptation of units, acts like training an ensemble of thinned networks, reduces reliance on any single neuron and improves robustness.\n",
      "- Early stopping\n",
      "  - Stop training when validation loss stops improving. Prevents the model from continuing to fit noise in the training set.\n",
      "- Data augmentation (often used as regularization)\n",
      "  - Increasing input variety forces the model to learn invariances instead of memorizing examples.\n",
      "- Output/label regularization\n",
      "  - Label smoothing discourages overconfident predictions and yields softer, better-calibrated outputs that generalize better.\n",
      "- Architectural and norm-based constraints\n",
      "  - Max-norm or clipping, spectral norm constraints, and limiting network width/depth reduce capacity and stabilize training.\n",
      "- Implicit regularization (optimization and normalization)\n",
      "  - Optimizers, batch normalization, and training procedures can implicitly regularize (e.g., SGD noise tends to prefer flatter minima).\n",
      "\n",
      "Intuition in one line\n",
      "- Regularization reduces the model’s ability to fit spurious patterns in the training data (reduces variance) while preserving its ability to capture the true signal.\n",
      "\n",
      "Practical guidance\n",
      "- Start with weight decay (e.g., 1e-4 to 1e-3 for many CNNs) and data augmentation for vision tasks.\n",
      "- Use dropout (0.1–0.5) in fully connected layers or where overfitting is observed; use cautiously with batch norm (they interact).\n",
      "- Monitor validation metrics and use early stopping based on the validation set.\n",
      "- Combine methods: e.g., weight decay + augmentation + early stopping (and dropout only where appropriate).\n",
      "- Tune regularization strength via validation or cross-validation; too much regularization causes underfitting (high bias).\n",
      "\n",
      "Summary\n",
      "- Regularization constrains the hypothesis space or training dynamics so the network learns simpler, smoother, and more robust functions, which reduces variance and helps the model generalize to unseen data.\n"
     ]
    }
   ],
   "source": [
    "print(policy_results[5]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da76cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
